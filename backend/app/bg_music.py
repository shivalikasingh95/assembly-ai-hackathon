from .album_cover import generate_album_cover_art
import eyed3
from eyed3.id3.frames import ImageFrame
import sys
import os
sys.path.append("../ml/")
from wandb_load_model import get_music_gen_model
from SymphonyNet.src.fairseq.gen_utils import (
    process_prime_midi, 
    gen_one, get_trk_ins_map, 
    get_note_seq, note_seq_to_midi_file, 
    music_dict
)
from fairseq.models import FairseqLanguageModel
import time
import boto3


prompt = "Dramatic, Dark, Super-Resolution, Evil, Neon Lamp, Cinematic Lighting, Chromatic Aberration, insanely detailed and intricate, hypermaximalist, elegant, ornate, hyper realistic, super detailed, Unreal Engine"

def generate_background_music(mid_file):
    # insert replicate model here
    mp3 = "mp3"
    mp3 = add_album_cover(prompt)
    return {"output_bg_music": mp3}


def add_album_cover(mp3):
    # use gpt3 for prompt gen maybe
    album_cover = generate_album_cover_art(prompt)
    audiofile = eyed3.load(mp3)
    if (audiofile.tag == None):
        audiofile.initTag()
    audiofile.tag.images.set(ImageFrame.FRONT_COVER, open(album_cover, 'rb').read(), 'image/jpeg')
    audiofile.tag.save()
    return audiofile


def generate_music(config: dict, midi_file_path: str):
    """
    This function calls the SymphonyNet model & generates the output music corresponding to given input midi file.
    Args:
        config (dict): config dictionary for music generation task
        midi_file_path (str): path of input midi file for the model
    Returns:
        mp3_output_name (str): path of mp3 file generated corresponding to model predicted note sequence
    """
    
    ## set up config for model
    MAX_POS_LEN = config["max_pos_len"]
    PI_LEVEL = config["pi_level"]
    IGNORE_META_LOSS = config["ignore_meta_loss"]
    RATIO = config["ratio"]
    BPE = config["bpe"]
    DATA_BIN=f"linear_{MAX_POS_LEN}_chord{BPE}_hardloss{IGNORE_META_LOSS}"
    CHECKPOINT_SUFFIX=f"{DATA_BIN}_PI{PI_LEVEL}"
    DATA_BIN_DIR=f"..ml/SymphonyNet/data/model_spec/{DATA_BIN}/bin/"
    DATA_VOC_DIR=f"..ml/SymphonyNet/data/model_spec/{DATA_BIN}/vocabs/"

    
    music_dict.load_vocabs_bpe(DATA_VOC_DIR, '..ml/SymphonyNet/data/bpe_res/' if BPE == '_bpe' else None)
    
    ## download model checkpoint from Weights & Biases model registry
    model_path = get_music_gen_model(config['wandb_project_name'])

    ## Load SymphonyNet model using checkpoint file path
    custom_lm = FairseqLanguageModel.from_pretrained('.', 
        checkpoint_file=model_path, 
        data_name_or_path=DATA_BIN_DIR, 
        user_dir="../ml/SymphonyNet/src/fairseq/linear_transformer_inference"
    )
    
    ## put model in eval mode for prediction
    music_gen_model = custom_lm.models[0]
    music_gen_model.cuda()
    music_gen_model.eval()

    ## preprocess input midi file before feeding to model for prediction
    max_measure_cnt = config["max_measure_cnt"]
    max_chord_measure_cnt = config["max_chord_measure_cnt"]
    prime, ins_label = process_prime_midi(midi_file_path, max_measure_cnt, max_chord_measure_cnt)
    
    ## generate note sequence output using SymphonyNet model
    while(True):
        try:
            generated, ins_logits = gen_one(music_gen_model, prime, MIN_LEN = 1024)
            break
        except Exception as e:
            print(e)
            continue
    trk_ins_map = get_trk_ins_map(generated, ins_logits)
    note_seq = get_note_seq(generated, trk_ins_map)
    
    ## generate midi file corresponding generated note sequence by model
    timestamp = time.strftime("%m-%d_%H-%M-%S", time.localtime()) 
    midi_output_name = config["midi_out_folder"] + f'output_prime{max_measure_cnt}_chord{max_chord_measure_cnt}_{timestamp}.mid'
    note_seq_to_midi_file(note_seq, midi_output_name)

    ## finally convert midi file generated by model to mp3
    mp3_output_name = config["mp3_out_folder"] + f'output_prime{max_measure_cnt}_chord{max_chord_measure_cnt}_{timestamp}.mp3'
    convert_midi_to_mp3(midi_output_name, mp3_output_name)

    ## upload mp3 file to s3
    upload_file_to_s3(mp3_file_path, config['s3_bucket_name'])

    if config["delete_audio_files"]:
        ## delete local audio files
        delete_local_audio_files(midi_output_name, mp3_output_name)

    return {"output_bg_music": mp3_output_name}


def convert_midi_to_mp3(midi_file_path: str, mp3_file_path: str, sampling_rate: int):
    """
    This function helps to convert a given midi file to its corresponding mp3 file
    Args:
        midi_file_path (str): path of midi file that needs to be converted 
        mp3_file_path (str): path of mp3 file generated after conversion
        sampling_rate (int): sampling rate to be used for converting audio
    """
    fs = FluidSynth('sound_font/MuseScore_General.sf2',sample_rate=sampling_rate)
    fs.midi_to_audio(midi_file_path, mp3_file_path)


def upload_file_to_s3(mp3_file_path:str, s3_bucket_name: str):
    """
    This function meant for uploading a given mp3 file to s3
    Args:
        mp3_file_path (str): path of mp3 file to be uploaded to s3
        s3_bucket_name (str): name of s3 bucket to be used for uploading
    """
    # Create an S3 access object
    s3 = boto3.client("s3")
    s3.upload_file(
        Filename=mp3_file_path,
        Bucket=s3_bucket_name,
    )

def delete_local_audio_files(midi_file_path: str, mp3_file_path: str):
    """
    This function helps to delete midi & mp3 files generated locally after upload to s3
    Args:
        midi_file_path (str): path of midi file that needs to be deleted
        mp3_file_path (str): path of mp3 file that needs to be deleted
    """

    os.remove(midi_file_path)
    os.remove(mp3_file_path)
